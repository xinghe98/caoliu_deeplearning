"""
========================================
è§†é¢‘å¸å¼•åŠ›é¢„æµ‹æ¨¡å‹ - ä¸»è®­ç»ƒè„šæœ¬
========================================

ä½¿ç”¨æ–¹æ³•ï¼š
    python train.py              # ä»å¤´å¼€å§‹è®­ç»ƒ
    python train.py --resume     # ç»§ç»­ä¸Šæ¬¡çš„è®­ç»ƒ
    python train.py --resume --epochs 30  # ç»§ç»­è®­ç»ƒå¹¶è®¾ç½®æ€»epochæ•°ä¸º30

ä½œè€…: AI Assistant
æ—¥æœŸ: 2026-01-31
"""

import os
import argparse
import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, WeightedRandomSampler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from transformers import BertTokenizer
import numpy as np
import pandas as pd

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from config import Config, get_device
from dataset import load_dataset, VideoDataset
from models import MultiModalClassifier
from utils import train_one_epoch, validate, set_seed, plot_training_history
from utils import FocalLoss, CombinedLoss, get_class_weights


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    """
    parser = argparse.ArgumentParser(description='è§†é¢‘å¸å¼•åŠ›é¢„æµ‹æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--resume', action='store_true', 
                        help='ç»§ç»­ä¸Šæ¬¡çš„è®­ç»ƒ')
    parser.add_argument('--epochs', type=int, default=None,
                        help='è®­ç»ƒçš„æ€»epochæ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--lr', type=float, default=None,
                        help='å­¦ä¹ ç‡ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--batch-size', type=int, default=None,
                        help='æ‰¹æ¬¡å¤§å°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    return parser.parse_args()


def main():
    """
    ä¸»å‡½æ•°ï¼šå®Œæ•´çš„è®­ç»ƒæµç¨‹
    """
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    print("="*60)
    print("       è§†é¢‘å¸å¼•åŠ›é¢„æµ‹æ¨¡å‹ - å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ è®­ç»ƒ")
    print("="*60)
    
    # è·å–è®¾å¤‡
    device = get_device()
    
    # åˆå§‹åŒ–é…ç½®
    config = Config()
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.epochs is not None:
        config.NUM_EPOCHS = args.epochs
    if args.lr is not None:
        config.LEARNING_RATE = args.lr
    if args.batch_size is not None:
        config.BATCH_SIZE = args.batch_size
    
    # æ˜¾ç¤ºè®­ç»ƒæ¨¡å¼
    if args.resume:
        print("\nğŸ“‚ æ¨¡å¼: ç»§ç»­è®­ç»ƒ (Resume Training)")
    else:
        print("\nğŸ†• æ¨¡å¼: ä»å¤´å¼€å§‹è®­ç»ƒ (Training from Scratch)")
    
    # è®¾ç½®éšæœºç§å­
    set_seed(config.RANDOM_SEED)
    
    # ==================== 1. åŠ è½½æ•°æ® ====================
    print("\n[1/6] åŠ è½½æ•°æ®é›†...")
    df = load_dataset(config)
    
    # ==================== 2. åˆ’åˆ†æ•°æ®é›† ====================
    print("\n[2/6] åˆ’åˆ†æ•°æ®é›†...")
    train_df, val_df = train_test_split(
        df, 
        test_size=0.2,           # 20%ä½œä¸ºéªŒè¯é›†
        stratify=df['label'],    # åˆ†å±‚é‡‡æ ·ï¼Œä¿æŒæ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
        random_state=config.RANDOM_SEED
    )
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_df)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_df)}")
    
    # ==================== 3. åˆå§‹åŒ–åˆ†è¯å™¨ ====================
    print("\n[3/6] åŠ è½½BERTåˆ†è¯å™¨...")
    tokenizer = BertTokenizer.from_pretrained(config.BERT_MODEL_NAME)
    print(f"âœ“ å·²åŠ è½½ {config.BERT_MODEL_NAME} åˆ†è¯å™¨")
    
    # ==================== 4. åˆ›å»ºæ•°æ®åŠ è½½å™¨ ====================
    print("\n[4/6] åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_dataset = VideoDataset(train_df, config, tokenizer, is_training=True)
    val_dataset = VideoDataset(val_df, config, tokenizer, is_training=False)
    
    # åˆ›å»ºåŠ æƒé‡‡æ ·å™¨ï¼ˆå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼‰
    if config.USE_WEIGHTED_SAMPLER:
        train_labels = train_df['label'].values
        sample_weights = get_class_weights(train_labels, device)
        sampler = WeightedRandomSampler(
            weights=sample_weights,
            num_samples=len(sample_weights),
            replacement=True
        )
        print(f"âœ“ ä½¿ç”¨åŠ æƒé‡‡æ ·å™¨å¹³è¡¡ç±»åˆ«")
        print(f"  æ­£æ ·æœ¬æƒé‡: {sample_weights[train_labels == 1].mean():.4f}")
        print(f"  è´Ÿæ ·æœ¬æƒé‡: {sample_weights[train_labels == 0].mean():.4f}")
        shuffle_train = False  # ä½¿ç”¨sampleræ—¶ä¸èƒ½shuffle
    else:
        sampler = None
        shuffle_train = True
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=shuffle_train,
        sampler=sampler,
        num_workers=0,  # Macä¸Šå»ºè®®è®¾ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜
        pin_memory=True if device.type == 'cuda' else False
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=0,
        pin_memory=True if device.type == 'cuda' else False
    )
    print(f"âœ“ è®­ç»ƒé›†æ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"âœ“ éªŒè¯é›†æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    # ==================== 5. åˆå§‹åŒ–æ¨¡å‹ ====================
    print("\n[5/6] åˆå§‹åŒ–æ¨¡å‹...")
    model = MultiModalClassifier(config).to(device)
    
    # æ‰“å°æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹æ€»å‚æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
    
    # å®šä¹‰æŸå¤±å‡½æ•°
    if config.USE_FOCAL_LOSS:
        # ä½¿ç”¨Focal Loss + Label Smoothingï¼ˆæ›´å¥½åœ°å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼‰
        criterion = CombinedLoss(
            alpha=config.FOCAL_ALPHA,
            gamma=config.FOCAL_GAMMA,
            smoothing=config.LABEL_SMOOTHING,
            focal_weight=0.7,
            bce_weight=0.3
        )
        print(f"âœ“ ä½¿ç”¨ Combined Loss (Focal + Label Smoothing)")
        print(f"  Focal alpha: {config.FOCAL_ALPHA}, gamma: {config.FOCAL_GAMMA}")
        print(f"  Label smoothing: {config.LABEL_SMOOTHING}")
    else:
        # ä½¿ç”¨å¸¦æƒé‡çš„äºŒå…ƒäº¤å‰ç†µ
        pos_weight = torch.tensor(
            [(df['label'] == 0).sum() / (df['label'] == 1).sum()],
            dtype=torch.float32,
            device=device
        )
        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        print(f"âœ“ ä½¿ç”¨ BCEWithLogitsLoss, æ­£æ ·æœ¬æƒé‡: {pos_weight.item():.2f}")
    
    # å®šä¹‰ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.LEARNING_RATE,
        weight_decay=config.WEIGHT_DECAY
    )
    
    # å®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=0.5,
        patience=2
    )
    
    # ==================== 6. å¼€å§‹è®­ç»ƒ ====================
    print("\n[6/6] å¼€å§‹è®­ç»ƒ...")
    print("="*60)
    
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []
    
    best_val_loss = float('inf')
    patience_counter = 0
    start_epoch = 0
    
    # å¦‚æœé€‰æ‹©ç»§ç»­è®­ç»ƒï¼ŒåŠ è½½checkpoint
    checkpoint_path = os.path.join(config.DATA_DIR, config.MODEL_SAVE_PATH)
    if args.resume:
        if os.path.exists(checkpoint_path):
            print("\næ­£åœ¨åŠ è½½checkpoint...")
            checkpoint = torch.load(checkpoint_path, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            best_val_loss = checkpoint['val_loss']
            print(f"âœ“ æˆåŠŸåŠ è½½checkpoint!")
            print(f"  - ä» epoch {start_epoch + 1} ç»§ç»­è®­ç»ƒ")
            print(f"  - ä¹‹å‰æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
            print(f"  - ä¹‹å‰æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {checkpoint['val_acc']*100:.2f}%")
        else:
            print(f"\nâš ï¸ æœªæ‰¾åˆ°checkpointæ–‡ä»¶: {checkpoint_path}")
            print("   å°†ä»å¤´å¼€å§‹è®­ç»ƒ...")
    
    print(f"\nè®¡åˆ’è®­ç»ƒ epochs: {start_epoch + 1} -> {config.NUM_EPOCHS}")
    
    for epoch in range(start_epoch, config.NUM_EPOCHS):
        print(f"\nEpoch {epoch+1}/{config.NUM_EPOCHS}")
        print("-" * 40)
        
        # è®­ç»ƒ
        train_loss, train_acc = train_one_epoch(
            model, train_loader, criterion, optimizer, device, epoch
        )
        
        # éªŒè¯
        val_loss, val_acc, predictions, labels, probs, video_ids, titles = validate(
            model, val_loader, criterion, device
        )
        
        # å½“å‡†ç¡®ç‡è¾¾åˆ°70%æ—¶ï¼Œä¿å­˜é”™è¯¯æ ·æœ¬
        if val_acc >= 0.70:
            # æ‰¾å‡ºé”™è¯¯æ ·æœ¬
            error_indices = np.where(predictions != labels)[0]
            
            if len(error_indices) > 0:
                error_data = []
                for idx in error_indices:
                    error_data.append({
                        'video_id': video_ids[idx],
                        'title': titles[idx],
                        'true_label': int(labels[idx]),
                        'pred_label': int(predictions[idx]),
                        'pred_prob': float(probs[idx]),
                        'epoch': epoch + 1
                    })
                
                error_df = pd.DataFrame(error_data)
                save_path = os.path.join(config.DATA_DIR, f"error_cases_epoch_{epoch+1}.csv")
                error_df.to_csv(save_path, index=False, encoding='utf-8-sig')
                print(f"  â˜… å‡†ç¡®ç‡è¾¾æ ‡ ({val_acc*100:.2f}%)ï¼Œå·²ä¿å­˜ {len(error_df)} ä¸ªé”™è¯¯æ ·æœ¬åˆ° error_cases_epoch_{epoch+1}.csv")
        
        # è®°å½•å†å²
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(val_loss)
        
        print(f"\n  è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒå‡†ç¡®ç‡: {train_acc*100:.2f}%")
        print(f"  éªŒè¯æŸå¤±: {val_loss:.4f} | éªŒè¯å‡†ç¡®ç‡: {val_acc*100:.2f}%")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'val_acc': val_acc,
            }, os.path.join(config.DATA_DIR, config.MODEL_SAVE_PATH))
            print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Loss: {val_loss:.4f})")
        else:
            patience_counter += 1
            print(f"  ! éªŒè¯æŸå¤±æœªæ”¹å–„ ({patience_counter}/{config.EARLY_STOPPING_PATIENCE})")
        
        # æ—©åœ
        if patience_counter >= config.EARLY_STOPPING_PATIENCE:
            print(f"\næ—©åœ: éªŒè¯æŸå¤±è¿ç»­{config.EARLY_STOPPING_PATIENCE}ä¸ªepochæœªæ”¹å–„")
            break
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_history(
        train_losses, val_losses, train_accs, val_accs,
        save_path=os.path.join(config.DATA_DIR, "training_history.png")
    )
    
    # ==================== æœ€ç»ˆè¯„ä¼° ====================
    print("\n" + "="*60)
    print("æœ€ç»ˆè¯„ä¼°ï¼ˆä½¿ç”¨æœ€ä½³æ¨¡å‹ï¼‰")
    print("="*60)
    
    checkpoint = torch.load(os.path.join(config.DATA_DIR, config.MODEL_SAVE_PATH))
    model.load_state_dict(checkpoint['model_state_dict'])
    
    _, final_acc, final_predictions, final_labels, _, _, _ = validate(
        model, val_loader, criterion, device
    )
    
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(
        final_labels, final_predictions,
        target_names=['ä¸å¥½çœ‹ (0)', 'å¥½çœ‹ (1)']
    ))
    
    print("\næ··æ·†çŸ©é˜µ:")
    print(confusion_matrix(final_labels, final_predictions))
    
    print("\n" + "="*60)
    print("è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {checkpoint['val_acc']*100:.2f}%")
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {os.path.join(config.DATA_DIR, config.MODEL_SAVE_PATH)}")
    print("="*60)


if __name__ == "__main__":
    main()
